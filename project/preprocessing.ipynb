import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import import_ipynb
import readdata

from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential, load_model, predict
from tensorflow.keras.layers import SimpleRNN,LSTM, Dense, RepeatVector, Masking, TimeDistributed, Bidirectional, Embedding
from tensorflow. keras.utils import plot_model
from tensorflow.keras.losses import MeanSquaredError
import tensorflow as tf
import pprint

pp=pprint.PrettyPrinter(indent=4)

groupset=[]
over_100_group=[]
for i in range(len(readdata.normal)):
    gb = readdata.normal[i].groupby(['_ws.col.Protocol','ip.src','ip.dst','tcp.srcport','tcp.dstport'])
    for key, group in gb:
        group = np.asarray(group)
        if len(group)>100:
            over_100_group.append(group)
        else :
            groupset.append(group)
        

def splitlen(array):
    split=array[:100,:]
    return split


for i in range(len(over_100_group)):
    groupset.append(splitlen(over_100_group[i]))

len(groupset)

X_sequnce=[]
for i in range(len(groupset)):
    X_sequnce.append(len(groupset[i]))
    


for i in range(len(groupset)):
    if len(groupset[i])>100:
        print(i)

X=[]
for i in range(len(groupset)):
    temp=np.delete(groupset[i],[0,1,2,3,4],1)
    num=100-len(temp)
    X.append(np.pad(temp,((0,num),(0,0)),'constant'))


X_data=np.asarray(X)

len(X_data)

Y_data=[]
for i in range(len(X_data)):
    Y_data.append(0)

len(Y_data)

공격데이터

groupset_a=[]
over_100_group_a=[]
gb = readdata.attack.groupby(['_ws.col.Protocol','ip.src','ip.dst','tcp.srcport','tcp.dstport'])
for key, group in gb:
    group = np.asarray(group)
    if len(group)>100:
        over_100_group_a.append(group)
    else :
        groupset_a.append(group)


over_100_group_a

for i in range(len(groupset_a)):
    X_sequnce.append(len(groupset_a[i]))
    
seq_length_batch = np.array(X_sequnce)
print(len(X_sequnce))

groupset_a

X_a=[]
for i in range(len(groupset_a)):
    temp=np.delete(groupset_a[i],[0,1,2,3,4],1)
    num=100-len(temp)
    X_a.append(np.pad(temp,((0,num),(0,0)),'constant'))

print(len(X_a),len(X_a[300]))

Y_a=[]
for i in range(len(X_a)):
    Y_a.append(1)

print(len(Y_a),Y_a[300])

X_attack_data=np.asarray(X_a)
Y_attack_data=np.asarray(Y_a)

X_total=np.concatenate((X_data,X_attack_data), axis=0)
#index : 0~891936 까지 normal, 총 891937

X_total.shape

Y_total=np.concatenate((Y_data,Y_attack_data), axis=0)

Y_total.shape

X_train, X_test, Y_train, Y_test = train_test_split(X_total,Y_total, test_size=0.3, shuffle=True ,random_state=101)

X_train.shape

X_test.shape

Y_train.shape

Y_test.shape

# cell = tf.contrib.rnn.BasicLSTMCell(num_units=5, state_is_tuple=True)
# outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32,
#                                          sequence_length=[1, 3, 2])
# lentgh 1 for batch 1, lentgh 2 for batch 2
    
# print("dynamic rnn: ", outputs)
# sess.run(tf.global_variables_initializer())
# pp.pprint(outputs.eval())  # batch size, unrolling (time), hidden_size

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)

learning_rate = 0.01
seq_length = 100
data_dim = 3
model = Sequential()
# model.add(Embedding(3, 3, input_length = 100, mask_zero=True))
model.add(LSTM(units=1, batch_input_shape=(None, 100, data_dim)))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate), loss='mean_squared_error', metrics=['acc'])

history = model.fit(X_train, Y_train, epochs=4, batch_size=None, validation_split=0.2)

model.summary()

loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=None)

print('## evaluation loss and_metrics ##')
print(loss_and_metrics)

# Plot predictions
plt.plot(Y_test)
plt.plot(loss_and_metrics)
plt.xlabel("Time Period")
plt.ylabel("Stock Price")
plt.show()





model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
history = model.fit(X_train, Y_train, epochs=1000, batch_size=64, validation_split=0.2)
